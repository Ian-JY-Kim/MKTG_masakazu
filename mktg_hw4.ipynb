{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A. Simulate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define True Parameters\n",
    "alpha_1, alpha_2, alpha_3, alpha_4, alpha_5 = (4, 3, 4.5, 2, 3)\n",
    "alpha_list = [alpha_1, alpha_2, alpha_3, alpha_4, alpha_5]\n",
    "gamma, beta_bar, sigma_beta = (1.5, -1, 0.2)\n",
    "omega_0, omega_1, omega_2, omega_3 = (4, 1, 1, 0.5)\n",
    "sigma_xi, sigma_e = (1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the length of the data\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "# panel frame t X j \n",
    "t_list = []\n",
    "j_list = []\n",
    "for i in range(1,T+1):\n",
    "    t_temp = list(np.full(5,i))\n",
    "    j_temp = [k for k in range(1,6)]\n",
    "    t_list = t_list + t_temp\n",
    "    j_list = j_list + j_temp\n",
    "\n",
    "# (1) Random Draw Part\n",
    "# fix the seed to allow replication\n",
    "np.random.seed(20220702)  #2, 7, 245787696, 20220702\n",
    "\n",
    "# draw x\n",
    "x_list = np.random.normal(1, 0.5, 5*T)\n",
    "\n",
    "# draw z_1, z_2\n",
    "z_1_list = np.random.normal(1, 0.5, 5*T)\n",
    "z_2_list = np.random.normal(1, 0.5, 5*T)\n",
    "\n",
    "# draw xi & e\n",
    "xi_list = np.random.normal(0, 1, 5*T)\n",
    "e_list = np.random.normal(0, 0.5, 5*T)\n",
    "\n",
    "# construct a dataframe\n",
    "df_master = df({\"t\": t_list, \"j\": j_list, \"x\": x_list, \"z_1\": z_1_list, \"z_2\": z_2_list, \"xi\": xi_list, \"e\": e_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate optimal price (endogenize price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the random draw of nu\n",
    "np.random.seed(2)\n",
    "nu_perseon_1000 = np.random.normal(0,1,1000)\n",
    "\n",
    "def share_partial_calculator(price_vector, work_data):\n",
    "    for k in range(5):\n",
    "        exp_list = []\n",
    "        exp_sum = 0\n",
    "        for k in range(5):\n",
    "            x_k = work_data['x'].values[k]\n",
    "            p_k = price_vector[k]\n",
    "            xi_k = work_data['xi'].values[k]\n",
    "            u_k_true = alpha_list[k] + gamma*x_k + beta_bar*p_k + xi_k + sigma_beta*p_k*nu_perseon_1000        \n",
    "            exp_list.append(np.exp(u_k_true))\n",
    "            exp_sum += np.exp(u_k_true)\n",
    "        \n",
    "        denominator = 1+exp_sum\n",
    "        share_list = [(exp_list[j]/denominator).mean() for j in range(5)]\n",
    "        partial_list = [((beta_bar + sigma_beta*nu_perseon_1000)*(exp_list[j]/denominator)*(1-(exp_list[j]/denominator))).mean() for j in range(5)]\n",
    "        return np.array(share_list), np.array(partial_list)\n",
    "\n",
    "\n",
    "## calculate price using fixed point algorithm \n",
    "price_list = []\n",
    "for t in range(1,T+1):    \n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "    epsilon = 1e-14\n",
    "    price_pre = np.array([4,4,4,4,4])\n",
    "    mc_array = omega_0 + omega_1*work_data['x'].values + omega_2*work_data['z_1'].values + omega_3*work_data['z_2'].values + work_data['e'].values\n",
    "\n",
    "    while True:\n",
    "        # calculate share and partial derivative \n",
    "        share_array = share_partial_calculator(price_pre, work_data)[0]\n",
    "        partial_array = share_partial_calculator(price_pre, work_data)[1]\n",
    "\n",
    "        # calculate post price\n",
    "        price_post = mc_array - share_array/partial_array\n",
    "\n",
    "        # break or update\n",
    "        if (abs(price_post - price_pre)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            price_pre = price_post\n",
    "            \n",
    "    price_list = price_list + list(price_post)\n",
    "    \n",
    "df_master['price'] = price_list\n",
    "\n",
    "\n",
    "\n",
    "## calculate associated market share\n",
    "true_share_list = []\n",
    "for t in range(1,T+1):    \n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "    true_share_temp = list(share_partial_calculator(work_data['price'].values, work_data)[0])\n",
    "    true_share_list = true_share_list + true_share_temp\n",
    "df_master['true_share'] = true_share_list\n",
    "\n",
    "\n",
    "## calculate true delta\n",
    "true_delta_list = []\n",
    "for t in range(1,T+1):\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "    true_delta_temp = []\n",
    "    for k in range(5):\n",
    "        x_k = work_data['x'].values[k]\n",
    "        p_k = work_data['price'].values[k]\n",
    "        xi_k = work_data['xi'].values[k]\n",
    "        alpha_k = alpha_list[k]\n",
    "        true_delta_k = alpha_k + gamma*x_k + beta_bar*p_k + xi_k\n",
    "        true_delta_temp.append(true_delta_k)\n",
    "    true_delta_list = true_delta_list + true_delta_temp\n",
    "df_master['true_delta'] = true_delta_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B. Demand Side Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from linearmodels.iv import IV2SLS\n",
    "import statsmodels.api as sm\n",
    "import linearmodels\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct IV\n",
    "df_avrg_x = df(df_master.groupby('t')['x'].mean())\n",
    "df_avrg_z_1 = df(df_master.groupby('t')['z_1'].mean())\n",
    "df_avrg_z_2 = df(df_master.groupby('t')['z_2'].mean())\n",
    "df_avrg_x = df_avrg_x.rename(columns= {\"x\": \"avrg_x\"})\n",
    "df_avrg_z_1 = df_avrg_z_1.rename(columns= {\"z_1\": \"avrg_z_1\"})\n",
    "df_avrg_z_2 = df_avrg_z_2.rename(columns= {\"z_2\": \"avrg_z_2\"})\n",
    "df_avrg_x = df_avrg_x.reset_index()\n",
    "df_avrg_z_1 = df_avrg_z_1.reset_index()\n",
    "df_avrg_z_2 = df_avrg_z_2.reset_index()\n",
    "\n",
    "df_master = pd.merge(df_master, df_avrg_x, left_on= \"t\", right_on= \"t\")\n",
    "df_master = pd.merge(df_master, df_avrg_z_1, left_on= \"t\", right_on= \"t\")\n",
    "df_master = pd.merge(df_master, df_avrg_z_2, left_on= \"t\", right_on= \"t\")\n",
    "\n",
    "df_master['IV2'] = (5*df_master['avrg_x'] - df_master['x'])/4\n",
    "df_master['IV3'] = (5*df_master['avrg_z_1'] - df_master['z_1'])/4\n",
    "df_master['IV4'] = (5*df_master['avrg_z_1'] - df_master['z_2'])/4\n",
    "\n",
    "df_master['constant'] = np.ones(5*T)\n",
    "df_master['firm2_dummy'] = np.where(df_master['j'] == 2, 1, 0)\n",
    "df_master['firm3_dummy'] = np.where(df_master['j'] == 3, 1, 0)\n",
    "df_master['firm4_dummy'] = np.where(df_master['j'] == 4, 1, 0)\n",
    "df_master['firm5_dummy'] = np.where(df_master['j'] == 5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_function(sigma_beta, df_master):\n",
    "\n",
    "    ############### Don't Change This Part #################\n",
    "    # code serching for delta Fixed Point \n",
    "    # coming from HW3, so this part is correct\n",
    "    epsilon = 1e-14\n",
    "    delta_list = []\n",
    "    for t in range(1, T+1):\n",
    "        work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "        # 1. set the initial vector of theta (name it delta_pre)\n",
    "        share_outer = 1 - work_data['true_share'].sum()\n",
    "        delta_pre = np.log(work_data['true_share'].values / share_outer) \n",
    "        \n",
    "        # 2. update delta by searching for the fixed point\n",
    "        #    calculate model predicted share with MC integration    \n",
    "        # fix the seed for random nu\n",
    "        np.random.seed(3)\n",
    "        nu_draw = np.random.normal(0, 1, 200)\n",
    "        while True:    \n",
    "            # 2-1. calculate denominator\n",
    "            exp_common = 0\n",
    "            for j in range(5):\n",
    "                delta_j = delta_pre[j]\n",
    "                p_j = work_data['price'].values[j]\n",
    "                exp_j = np.exp(delta_j + sigma_beta*p_j*nu_draw)     ##### sigma beta matters here #####\n",
    "                exp_common += exp_j\n",
    "            denominator_common = 1+ exp_common\n",
    "                \n",
    "            # 2-2. calculate market share for 10 products \n",
    "            share_list_temp = []\n",
    "            for j in range(5):\n",
    "                delta_j = delta_pre[j]\n",
    "                p_j = work_data['price'].values[j]\n",
    "                exp_j = np.exp(delta_j + sigma_beta*p_j*nu_draw)\n",
    "                s_j = (exp_j/denominator_common).mean()\n",
    "                share_list_temp.append(s_j)\n",
    "\n",
    "            # 2-3. update delta\n",
    "            share_list_array = np.array(share_list_temp)\n",
    "            delta_post = delta_pre + np.log(work_data['true_share'].values / share_list_array) \n",
    "\n",
    "            # 3. Break Condition and Update\n",
    "            if (abs(delta_post - delta_pre)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                delta_pre = delta_post\n",
    "\n",
    "        delta_list_temp = list(delta_pre)\n",
    "        delta_list = delta_list + delta_list_temp\n",
    "    \n",
    "    df_master['delta_FXP'] = delta_list \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #step 2: 2SLS\n",
    "    IV_result = IV2SLS(dependent=df_master['delta_FXP'],\n",
    "                    exog = df_master[['constant', 'firm2_dummy', 'firm3_dummy', 'firm4_dummy', 'firm5_dummy', 'x']],\n",
    "                    endog = df_master['price'],\n",
    "                    instruments=df_master[['z_1','z_2','IV2']]).fit()\n",
    "\n",
    "    alpha_1_hat, firm_2_dummy, firm_3_dummy, firm_4_dummy, firm_5_dummy = IV_result.params[:5]\n",
    "    alpha_2_hat = alpha_1_hat + firm_2_dummy\n",
    "    alpha_3_hat = alpha_1_hat + firm_3_dummy\n",
    "    alpha_4_hat = alpha_1_hat + firm_4_dummy\n",
    "    alpha_5_hat = alpha_1_hat + firm_5_dummy\n",
    "    alpha_hat_list = [alpha_1_hat, alpha_2_hat, alpha_3_hat, alpha_4_hat, alpha_5_hat]\n",
    "    gamma_hat = IV_result.params[5]\n",
    "    beta_bar_hat = IV_result.params[6]\n",
    "\n",
    "    xi_list = []\n",
    "    for t in range(1, T+1):\n",
    "        work_data = df_master.loc[df_master['t'] == t]\n",
    "        xi_temp = []\n",
    "        for k in range(5):\n",
    "            delta_FXP_k = df_master['delta_FXP'].values[k]\n",
    "            alpha_k = alpha_hat_list[k]\n",
    "            x_k = df_master['x'].values[k]\n",
    "            price_k = df_master['price'].values[k]\n",
    "            xi_k = delta_FXP_k - alpha_k - gamma_hat*x_k - beta_bar_hat*price_k\n",
    "            xi_temp.append(xi_k)\n",
    "        xi_list = xi_list + xi_temp\n",
    "\n",
    "    xi_array = np.array(xi_list)\n",
    "    \n",
    "    \n",
    "    #step 3: GMM\n",
    "    #sample analogy\n",
    "    g1_bar = (xi_array * df_master['z_1'].values).mean()\n",
    "    g2_bar = (xi_array * df_master['z_2'].values).mean()\n",
    "    g3_bar = (xi_array * df_master['IV2'].values).mean()\n",
    "    g4_bar = ((xi_array * df_master['z_1'].values)**2).mean() \n",
    "    g5_bar = ((xi_array * df_master['z_2'].values)**2).mean()\n",
    "    g6_bar = ((xi_array * df_master['IV2'].values)**2).mean()\n",
    "\n",
    "    g_bar_array = np.array([g1_bar, g2_bar, g3_bar, g4_bar, g5_bar, g6_bar])\n",
    "    #g_bar_array = np.array([g1_bar, g2_bar, g4_bar, g5_bar])\n",
    "\n",
    "    return g_bar_array@g_bar_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 7.887872\n",
      "         Iterations: 14\n",
      "         Function evaluations: 28\n"
     ]
    }
   ],
   "source": [
    "GMM_result = minimize(obj_function, 0.5, args = (df_master), bounds = ((0,5),), method='Nelder-Mead', options={'maxiter':200, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM estimate for alpha_1:  4.427154928289852\n",
      "GMM estimate for alpha_2:  3.633556882859601\n",
      "GMM estimate for alpha_3:  5.037803792394429\n",
      "GMM estimate for alpha_4:  2.4005726552860995\n",
      "GMM estimate for alpha_5:  3.570424446084255\n",
      "GMM estimate for gamma:  1.814013352154717\n",
      "GMM estimate for beta_bar:  -1.1150626597026179\n",
      "GMM estimate for sigma_beta:  0.22910156249999974\n"
     ]
    }
   ],
   "source": [
    "sigma_beta_hat = GMM_result.x[0]\n",
    "############### Don't Change This Part #################\n",
    "# code serching for delta Fixed Point \n",
    "# coming from HW3, so this part is correct\n",
    "epsilon = 1e-14\n",
    "delta_list = []\n",
    "for t in range(1, T+1):\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "    # 1. set the initial vector of theta (name it delta_pre)\n",
    "    share_outer = 1 - work_data['true_share'].sum()\n",
    "    delta_pre = np.log(work_data['true_share'].values / share_outer) \n",
    "    \n",
    "    # 2. update delta by searching for the fixed point\n",
    "    #    calculate model predicted share with MC integration    \n",
    "    # fix the seed for random nu\n",
    "    np.random.seed(3)\n",
    "    nu_draw = np.random.normal(0, 1, 200)\n",
    "    while True:    \n",
    "        # 2-1. calculate denominator\n",
    "        exp_common = 0\n",
    "        for j in range(5):\n",
    "            delta_j = delta_pre[j]\n",
    "            p_j = work_data['price'].values[j]\n",
    "            exp_j = np.exp(delta_j + sigma_beta_hat*p_j*nu_draw)     ##### sigma beta matters here #####\n",
    "            exp_common += exp_j\n",
    "        denominator_common = 1+ exp_common\n",
    "            \n",
    "        # 2-2. calculate market share for 10 products \n",
    "        share_list_temp = []\n",
    "        for j in range(5):\n",
    "            delta_j = delta_pre[j]\n",
    "            p_j = work_data['price'].values[j]\n",
    "            exp_j = np.exp(delta_j + sigma_beta_hat*p_j*nu_draw)\n",
    "            s_j = (exp_j/denominator_common).mean()\n",
    "            share_list_temp.append(s_j)\n",
    "\n",
    "        # 2-3. update delta\n",
    "        share_list_array = np.array(share_list_temp)\n",
    "        delta_post = delta_pre + np.log(work_data['true_share'].values / share_list_array) \n",
    "\n",
    "        # 3. Break Condition and Update\n",
    "        if (abs(delta_post - delta_pre)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            delta_pre = delta_post\n",
    "\n",
    "    delta_list_temp = list(delta_pre)\n",
    "    delta_list = delta_list + delta_list_temp\n",
    "\n",
    "df_master['delta_FXP'] = delta_list \n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#step 2: 2SLS\n",
    "IV_result = IV2SLS(dependent=df_master['delta_FXP'],\n",
    "                exog = df_master[['constant', 'firm2_dummy', 'firm3_dummy', 'firm4_dummy', 'firm5_dummy', 'x']],\n",
    "                endog = df_master['price'],\n",
    "                instruments=df_master[['z_1','z_2','IV2']]).fit()\n",
    "\n",
    "alpha_1_hat, firm_2_dummy, firm_3_dummy, firm_4_dummy, firm_5_dummy = IV_result.params[:5]\n",
    "alpha_2_hat = alpha_1_hat + firm_2_dummy\n",
    "alpha_3_hat = alpha_1_hat + firm_3_dummy\n",
    "alpha_4_hat = alpha_1_hat + firm_4_dummy\n",
    "alpha_5_hat = alpha_1_hat + firm_5_dummy\n",
    "gamma_hat = IV_result.params[5]\n",
    "beta_bar_hat = IV_result.params[6]\n",
    "\n",
    "print(\"GMM estimate for alpha_1: \", alpha_1_hat)\n",
    "print(\"GMM estimate for alpha_2: \", alpha_2_hat)\n",
    "print(\"GMM estimate for alpha_3: \", alpha_3_hat)\n",
    "print(\"GMM estimate for alpha_4: \", alpha_4_hat)\n",
    "print(\"GMM estimate for alpha_5: \", alpha_5_hat)\n",
    "print(\"GMM estimate for gamma: \", gamma_hat)\n",
    "print(\"GMM estimate for beta_bar: \", beta_bar_hat)\n",
    "print(\"GMM estimate for sigma_beta: \", sigma_beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C. Supply Side Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the marginal cost\n",
    "df_master['mc_hat'] = df_master['price'].values + 1/(beta_bar_hat*(1-df_master['true_share'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS estimate for omega_0:  4.443804151285334\n",
      "OLS estimate for omega_1:  1.057734481890185\n",
      "OLS estimate for omega_2:  1.027430405897602\n",
      "OLS estimate for omega_3:  0.3506671992797803\n"
     ]
    }
   ],
   "source": [
    "# do the OLS\n",
    "mc_regression = sm.OLS(df_master['mc_hat'], df_master[['constant', 'x', 'z_1', 'z_2']]).fit()\n",
    "omega_0_hat = mc_regression.params[0]\n",
    "omega_1_hat = mc_regression.params[1]\n",
    "omega_2_hat = mc_regression.params[2]\n",
    "omega_3_hat = mc_regression.params[3]\n",
    "\n",
    "print(\"OLS estimate for omega_0: \", omega_0_hat)\n",
    "print(\"OLS estimate for omega_1: \", omega_1_hat)\n",
    "print(\"OLS estimate for omega_2: \", omega_2_hat)\n",
    "print(\"OLS estimate for omega_3: \", omega_3_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D. Counterfactual Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Brand Value Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP1 calculate xi_hat & e_hat\n",
    "df_alpha = df({'j': [1,2,3,4,5], 'alpha_hat': [alpha_1_hat, alpha_2_hat, alpha_3_hat, alpha_4_hat, alpha_5_hat]})\n",
    "df_master = pd.merge(df_master, df_alpha, left_on='j', right_on='j')\n",
    "df_master['xi_hat'] = df_master['delta_FXP'].values - df_master['alpha_hat'].values - gamma_hat*df_master['x'].values - beta_bar_hat*df_master['price'].values\n",
    "df_master['e_hat'] = df_master['mc_hat'].values - omega_0_hat - omega_1_hat*df_master['x'].values - omega_2_hat*df_master['z_1'].values - omega_3_hat*df_master['z_2'].values\n",
    "\n",
    "xi_hat_1 = df_master.loc[df_master['j'] == 1]['xi_hat'].values\n",
    "xi_hat_2 = df_master.loc[df_master['j'] == 2]['xi_hat'].values\n",
    "xi_hat_3 = df_master.loc[df_master['j'] == 3]['xi_hat'].values\n",
    "xi_hat_4 = df_master.loc[df_master['j'] == 4]['xi_hat'].values\n",
    "xi_hat_5 = df_master.loc[df_master['j'] == 5]['xi_hat'].values\n",
    "\n",
    "e_hat_1 = df_master.loc[df_master['j'] == 1]['e_hat'].values\n",
    "e_hat_2 = df_master.loc[df_master['j'] == 2]['e_hat'].values\n",
    "e_hat_3 = df_master.loc[df_master['j'] == 3]['e_hat'].values\n",
    "e_hat_4 = df_master.loc[df_master['j'] == 4]['e_hat'].values\n",
    "e_hat_5 = df_master.loc[df_master['j'] == 5]['e_hat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hat_list = [alpha_1_hat, alpha_2_hat, alpha_3_hat, alpha_4_hat, alpha_5_hat]\n",
    "\n",
    "\n",
    "def share_partial_calculator_baseline(price_vector, work_data):\n",
    "    exp_list = []\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data['x'].values[k]\n",
    "        p_k = price_vector[k]\n",
    "        xi_k = work_data['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_list.append(np.exp(u_k_true))\n",
    "        exp_sum += np.exp(u_k_true)\n",
    "    \n",
    "    denominator = 1+exp_sum\n",
    "    share_list = [(exp_list[j]/denominator).mean() for j in range(5)]\n",
    "    partial_list = [((beta_bar_hat + sigma_beta_hat*nu_perseon_1000)*(exp_list[j]/denominator)*(1-(exp_list[j]/denominator))).mean() for j in range(5)]\n",
    "    return np.array(share_list), np.array(partial_list)\n",
    "\n",
    "\n",
    "def share_partial_calculator_counterfactual(price_vector, work_data, j):   #j: 0,1,2,3,4\n",
    "    alpha_hat_list_prime = alpha_hat_list.copy()\n",
    "    alpha_hat_list_prime[j] = 0\n",
    "\n",
    "    exp_list = []\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data['x'].values[k]\n",
    "        p_k = price_vector[k]\n",
    "        xi_k = work_data['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list_prime[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_list.append(np.exp(u_k_true))\n",
    "        exp_sum += np.exp(u_k_true)\n",
    "    \n",
    "    denominator = 1+exp_sum\n",
    "    share_list = [(exp_list[j]/denominator).mean() for j in range(5)]\n",
    "    partial_list = [((beta_bar_hat + sigma_beta_hat*nu_perseon_1000)*(exp_list[j]/denominator)*(1-(exp_list[j]/denominator))).mean() for j in range(5)]\n",
    "    return np.array(share_list), np.array(partial_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterfactual_pi(j, t):\n",
    "    # STEP2&3 calculate the expected profit change\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "\n",
    "    delta_pi_j = []\n",
    "    eqm_price_BL = []\n",
    "    eqm_price_CF = []\n",
    "    R = 100\n",
    "    for r in range(R):\n",
    "        # randomly draw xi and e \n",
    "        xi_1_random = random.sample(list(xi_hat_1), 1)[0]\n",
    "        xi_2_random = random.sample(list(xi_hat_2), 1)[0]\n",
    "        xi_3_random = random.sample(list(xi_hat_3), 1)[0]\n",
    "        xi_4_random = random.sample(list(xi_hat_4), 1)[0]\n",
    "        xi_5_random = random.sample(list(xi_hat_5), 1)[0]\n",
    "\n",
    "        e_1_random = random.sample(list(e_hat_1), 1)[0]\n",
    "        e_2_random = random.sample(list(e_hat_2), 1)[0]\n",
    "        e_3_random = random.sample(list(e_hat_3), 1)[0]\n",
    "        e_4_random = random.sample(list(e_hat_4), 1)[0]\n",
    "        e_5_random = random.sample(list(e_hat_5), 1)[0]\n",
    "\n",
    "        xi_random_array = np.array([xi_1_random, xi_2_random, xi_3_random, xi_4_random, xi_5_random])\n",
    "        e_random_array = np.array([e_1_random, e_2_random, e_3_random, e_4_random, e_5_random])\n",
    "        pd.set_option('mode.chained_assignment',  None)\n",
    "        work_data['xi'] = xi_random_array\n",
    "        work_data['e'] = e_random_array\n",
    "\n",
    "        epsilon = 1e-14\n",
    "        price_pre_baseline = np.array([4,4,4,4,4])\n",
    "        price_pre_counterfactual = np.array([4,4,4,4,4])\n",
    "        mc_array = omega_0_hat + omega_1_hat*work_data['x'].values + omega_2_hat*work_data['z_1'].values + omega_3_hat*work_data['z_2'].values + work_data['e'].values\n",
    "\n",
    "        # (1) calculate baseline price: p^*\n",
    "        count = 0\n",
    "        while (count <= 50):\n",
    "            # calculate share and partial derivative\n",
    "            share_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[0]\n",
    "            partial_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[1]\n",
    "\n",
    "            # calculate post price\n",
    "            price_post_baseline = mc_array - share_array_baseline/partial_array_baseline\n",
    "\n",
    "            # break or update\n",
    "            if (abs(price_post_baseline - price_pre_baseline)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                price_pre_baseline = price_post_baseline\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        # (2) calculate counterfactual price: p^**\n",
    "        count = 0\n",
    "        while (count <= 50):\n",
    "            # calculate share and partial derivative\n",
    "            share_array_counterfactual = share_partial_calculator_counterfactual(price_pre_counterfactual, work_data, j)[0]\n",
    "            partial_array_counterfactual = share_partial_calculator_counterfactual(price_pre_counterfactual, work_data, j)[1]\n",
    "\n",
    "            # calculate post price\n",
    "            price_post_counterfactual = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
    "\n",
    "            # break or update\n",
    "            if (abs(price_post_counterfactual - price_pre_counterfactual)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                price_pre_counterfactual = price_post_counterfactual\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        \n",
    "        eqm_price_baseline = price_post_baseline\n",
    "        eqm_share_baseline = share_partial_calculator_baseline(eqm_price_baseline, work_data)[0]\n",
    "        eqm_pi_baseline = (eqm_price_baseline - mc_array)*eqm_share_baseline\n",
    "\n",
    "        eqm_price_counterfactual = price_post_counterfactual\n",
    "        eqm_share_counterfactual = share_partial_calculator_counterfactual(eqm_price_counterfactual, work_data, j)[0]\n",
    "        eqm_pi_counterfactual = (eqm_price_counterfactual - mc_array)*eqm_share_counterfactual\n",
    "\n",
    "        eqm_price_BL.append(eqm_price_baseline)\n",
    "        eqm_price_CF.append(eqm_price_counterfactual)\n",
    "        delta_pi_j.append((eqm_pi_counterfactual - eqm_pi_baseline)[j])\n",
    "\n",
    "\n",
    "    return np.array(delta_pi_j).mean(), np.array(eqm_price_BL).mean(0), np.array(eqm_price_CF).mean(0)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_calculator_pi(j):\n",
    "\n",
    "    T = 20\n",
    "    avrg_delta_pi_sum = 0\n",
    "    price_before_vec = []\n",
    "    price_after_vec = []\n",
    "    for t in range(1, T+1):\n",
    "        avrg_delta_pi, price_before, price_after = counterfactual_pi(j, t)\n",
    "        avrg_delta_pi_sum += avrg_delta_pi\n",
    "        price_before_vec.append(price_before)\n",
    "        price_after_vec.append(price_after)\n",
    "\n",
    "    return -avrg_delta_pi_sum/T, np.array(price_before_vec).mean(0), np.array(price_after_vec).mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "product5_brand_value, product5_pre_price, product5_post_price = outcome_calculator_pi(4)\n",
    "product4_brand_value, product4_pre_price, product4_post_price = outcome_calculator_pi(3)\n",
    "product3_brand_value, product3_pre_price, product3_post_price = outcome_calculator_pi(2)\n",
    "product2_brand_value, product2_pre_price, product2_post_price = outcome_calculator_pi(1)\n",
    "product1_brand_value, product1_pre_price, product1_post_price = outcome_calculator_pi(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand Value of Firm 1:  0.10763541195956221\n",
      "Brand Value of Firm 2:  0.06429564231771082\n",
      "Brand Value of Firm 3:  0.188268403565025\n",
      "Brand Value of Firm 4:  0.018154916728188204\n",
      "Brand Value of Firm 5:  0.04972323959187671\n"
     ]
    }
   ],
   "source": [
    "print(\"Brand Value of Firm 1: \", product1_brand_value)\n",
    "print(\"Brand Value of Firm 2: \", product2_brand_value)\n",
    "print(\"Brand Value of Firm 3: \", product3_brand_value)\n",
    "print(\"Brand Value of Firm 4: \", product4_brand_value)\n",
    "print(\"Brand Value of Firm 5: \", product5_brand_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_value = [product1_brand_value, product2_brand_value, product3_brand_value, product4_brand_value, product5_brand_value]\n",
    "x_tick = [\"Frim1\", \"Firm2\", \"Firm3\", \"Firm4\", \"Firm5\"]\n",
    "plt.bar(x_tick, b_value)\n",
    "plt.title(\"Brand Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.87573682  0.71372366  1.33606945  0.45426546  0.64129057]\n",
      "[ 0.53170212 -1.12206159  0.78357877  0.26115759  0.36562278]\n",
      "[ 1.68678082  1.310908   -3.18772293  0.79674109  1.08802876]\n",
      "[ 0.15610392  0.12556989  0.2200521  -0.33972462  0.10737512]\n",
      "[ 0.44100282  0.3125666   0.59780749  0.18031071 -0.91512679]\n"
     ]
    }
   ],
   "source": [
    "print(100*(product1_post_price-product1_pre_price)/product1_pre_price)\n",
    "print(100*(product2_post_price-product2_pre_price)/product2_pre_price)\n",
    "print(100*(product3_post_price-product3_pre_price)/product3_pre_price)\n",
    "print(100*(product4_post_price-product4_pre_price)/product4_pre_price)\n",
    "print(100*(product5_post_price-product5_pre_price)/product5_pre_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_after(xi, e, work_data, j):\n",
    "    '''\n",
    "    xi: sampled 5X1 vector of xi \n",
    "    e : sampled 5X1 vector of e\n",
    "    work_data: certain market t's data\n",
    "    j : counterfactual product\n",
    "    '''\n",
    "    alpha_hat_list_prime = alpha_hat_list.copy()\n",
    "    alpha_hat_list_prime[j] = 0\n",
    "\n",
    "    work_data_temp = work_data.copy()\n",
    "    work_data_temp['xi'] = xi\n",
    "    work_data_temp['e'] = e\n",
    "    \n",
    "    price_pre_counterfactual = np.array([4,4,4,4,4])\n",
    "    mc_array = omega_0_hat + omega_1_hat*work_data_temp['x'].values + omega_2_hat*work_data_temp['z_1'].values + omega_3_hat*work_data_temp['z_2'].values + work_data_temp['e'].values\n",
    "    \n",
    "    # (1) calculate eqm price under xi and e\n",
    "    count = 0\n",
    "    while (count <= 50):\n",
    "        # calculate share and partial derivative\n",
    "        share_array_counterfactual = share_partial_calculator_counterfactual(price_pre_counterfactual, work_data_temp, j)[0]\n",
    "        partial_array_counterfactual = share_partial_calculator_counterfactual(price_pre_counterfactual, work_data_temp, j)[1]\n",
    "\n",
    "        # calculate post price\n",
    "        price_post_counterfactual = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
    "\n",
    "        # break or update\n",
    "        if (abs(price_post_counterfactual - price_pre_counterfactual)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            price_pre_counterfactual = price_post_counterfactual\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    eqm_price = price_post_counterfactual\n",
    "    \n",
    "    # (2) calculate exponential sum\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data_temp['x'].values[k]\n",
    "        p_k = eqm_price[k]\n",
    "        xi_k = work_data_temp['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list_prime[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_sum += np.exp(u_k_true)\n",
    "    \n",
    "    # (3) calculate the average\n",
    "    numerator = np.log(1+exp_sum)\n",
    "    denominator = beta_bar_hat + sigma_beta_hat*nu_perseon_1000\n",
    "    CW_after = -(numerator/denominator).mean()\n",
    "\n",
    "    return CW_after\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_before(xi, e, work_data):\n",
    "    '''\n",
    "    xi: sampled 5X1 vector of xi \n",
    "    e : sampled 5X1 vector of e\n",
    "    work_data: certain market t's data\n",
    "    '''\n",
    "    alpha_hat_list_prime = alpha_hat_list.copy()\n",
    "\n",
    "    work_data_temp = work_data.copy()\n",
    "    work_data_temp['xi'] = xi\n",
    "    work_data_temp['e'] = e\n",
    "    \n",
    "    price_pre_baseline = np.array([4,4,4,4,4])\n",
    "    mc_array = omega_0_hat + omega_1_hat*work_data_temp['x'].values + omega_2_hat*work_data_temp['z_1'].values + omega_3_hat*work_data_temp['z_2'].values + work_data_temp['e'].values\n",
    "    \n",
    "    # (1) calculate eqm price under xi and e\n",
    "    count = 0\n",
    "    while (count <= 50):\n",
    "        # calculate share and partial derivative\n",
    "        share_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data_temp)[0]\n",
    "        partial_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data_temp)[1]\n",
    "\n",
    "        # calculate post price\n",
    "        price_post_baseline = mc_array - share_array_baseline/partial_array_baseline\n",
    "\n",
    "        # break or update\n",
    "        if (abs(price_post_baseline - price_pre_baseline)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            price_pre_baseline = price_post_baseline\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    eqm_price = price_post_baseline\n",
    "    \n",
    "    # (2) calculate exponential sum\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data_temp['x'].values[k]\n",
    "        p_k = eqm_price[k]\n",
    "        xi_k = work_data_temp['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list_prime[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_sum += np.exp(u_k_true)\n",
    "    \n",
    "    # (3) calculate the average\n",
    "    numerator = np.log(1+exp_sum)\n",
    "    denominator = beta_bar_hat + sigma_beta_hat*nu_perseon_1000\n",
    "    CW_before = -(numerator/denominator).mean()\n",
    "\n",
    "    return CW_before\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_change(j,t):\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "    CW_delta = 0\n",
    "    R = 100\n",
    "    for r in range(R):\n",
    "        # randomly draw xi and e \n",
    "        xi_1_random = random.sample(list(xi_hat_1), 1)[0]\n",
    "        xi_2_random = random.sample(list(xi_hat_2), 1)[0]\n",
    "        xi_3_random = random.sample(list(xi_hat_3), 1)[0]\n",
    "        xi_4_random = random.sample(list(xi_hat_4), 1)[0]\n",
    "        xi_5_random = random.sample(list(xi_hat_5), 1)[0]\n",
    "\n",
    "        e_1_random = random.sample(list(e_hat_1), 1)[0]\n",
    "        e_2_random = random.sample(list(e_hat_2), 1)[0]\n",
    "        e_3_random = random.sample(list(e_hat_3), 1)[0]\n",
    "        e_4_random = random.sample(list(e_hat_4), 1)[0]\n",
    "        e_5_random = random.sample(list(e_hat_5), 1)[0]\n",
    "\n",
    "        xi_random_array = np.array([xi_1_random, xi_2_random, xi_3_random, xi_4_random, xi_5_random])\n",
    "        e_random_array = np.array([e_1_random, e_2_random, e_3_random, e_4_random, e_5_random])\n",
    "        pd.set_option('mode.chained_assignment',  None)\n",
    "        \n",
    "        CW_post = CW_after(xi_random_array, e_random_array, work_data, j)\n",
    "        CW_pre = CW_before(xi_random_array, e_random_array, work_data)\n",
    "\n",
    "        CW_delta += (CW_post - CW_pre)\n",
    "\n",
    "    return CW_delta/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_change_calculator(j):\n",
    "    CW_change_list = [CW_change(j, t) for t in range(1,21)]\n",
    "    return np.array(CW_change_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_change_from_product1 = CW_change_calculator(0)\n",
    "CW_change_from_product2 = CW_change_calculator(1)\n",
    "CW_change_from_product3 = CW_change_calculator(2)\n",
    "CW_change_from_product4 = CW_change_calculator(3)\n",
    "CW_change_from_product5 = CW_change_calculator(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welfare change from killing product 1's BV:  -0.13330195734587252\n",
      "Welfare change from killing product 2's BV:  -0.08253585808949895\n",
      "Welfare change from killing product 3's BV:  -0.22004581497287895\n",
      "Welfare change from killing product 4's BV:  -0.02366081500115812\n",
      "Welfare change from killing product 5's BV:  -0.06166344159834476\n"
     ]
    }
   ],
   "source": [
    "print(\"Welfare change from killing product 1's BV: \", CW_change_from_product1)\n",
    "print(\"Welfare change from killing product 2's BV: \", CW_change_from_product2)\n",
    "print(\"Welfare change from killing product 3's BV: \", CW_change_from_product3)\n",
    "print(\"Welfare change from killing product 4's BV: \", CW_change_from_product4)\n",
    "print(\"Welfare change from killing product 5's BV: \", CW_change_from_product5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_elasticity(price_vector, work_data):\n",
    "    exp_list = []\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data['x'].values[k]\n",
    "        p_k = price_vector[k]\n",
    "        xi_k = work_data['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_list.append(np.exp(u_k_true))\n",
    "        exp_sum += np.exp(u_k_true)\n",
    "    \n",
    "    denominator = 1+exp_sum\n",
    "    cross_elasticity = -((beta_bar_hat + sigma_beta_hat*nu_perseon_1000)*(exp_list[0]/denominator)*(exp_list[1]/denominator)).mean()\n",
    "    \n",
    "    return cross_elasticity\n",
    "\n",
    "\n",
    "def cross_elasticity13(price_vector, work_data):\n",
    "    exp_list = []\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data['x'].values[k]\n",
    "        p_k = price_vector[k]\n",
    "        xi_k = work_data['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_list.append(np.exp(u_k_true))\n",
    "        exp_sum += np.exp(u_k_true)\n",
    "    \n",
    "    denominator = 1+exp_sum\n",
    "    cross_elasticity = -((beta_bar_hat + sigma_beta_hat*nu_perseon_1000)*(exp_list[0]/denominator)*(exp_list[2]/denominator)).mean()\n",
    "    \n",
    "    return cross_elasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pi12(t):\n",
    "    # STEP2&3 calculate the expected profit change\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "    delta_pi = []\n",
    "    eqm_price_before = []\n",
    "    eqm_price_after = []\n",
    "    \n",
    "    R = 100\n",
    "    for r in range(R):    \n",
    "        xi_1_random = random.sample(list(xi_hat_1), 1)[0]\n",
    "        xi_2_random = random.sample(list(xi_hat_2), 1)[0]\n",
    "        xi_3_random = random.sample(list(xi_hat_3), 1)[0]\n",
    "        xi_4_random = random.sample(list(xi_hat_4), 1)[0]\n",
    "        xi_5_random = random.sample(list(xi_hat_5), 1)[0]\n",
    "\n",
    "        e_1_random = random.sample(list(e_hat_1), 1)[0]\n",
    "        e_2_random = random.sample(list(e_hat_2), 1)[0]\n",
    "        e_3_random = random.sample(list(e_hat_3), 1)[0]\n",
    "        e_4_random = random.sample(list(e_hat_4), 1)[0]\n",
    "        e_5_random = random.sample(list(e_hat_5), 1)[0]\n",
    "\n",
    "        xi_random_array = np.array([xi_1_random, xi_2_random, xi_3_random, xi_4_random, xi_5_random])\n",
    "        e_random_array = np.array([e_1_random, e_2_random, e_3_random, e_4_random, e_5_random])\n",
    "        pd.set_option('mode.chained_assignment',  None)\n",
    "        work_data['xi'] = xi_random_array\n",
    "        work_data['e'] = e_random_array\n",
    "\n",
    "        epsilon = 1e-14\n",
    "        price_pre_baseline = np.array([4,4,4,4,4])\n",
    "        price_pre_counterfactual = np.array([4,4,4,4,4])\n",
    "        mc_array = omega_0_hat + omega_1_hat*work_data['x'].values + omega_2_hat*work_data['z_1'].values + omega_3_hat*work_data['z_2'].values + work_data['e'].values\n",
    "\n",
    "\n",
    "        # (1) calculate baseline price: p^*\n",
    "        count = 0\n",
    "        while (count <= 50):\n",
    "            # calculate share and partial derivative\n",
    "            share_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[0]\n",
    "            partial_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[1]\n",
    "\n",
    "            # calculate post price\n",
    "            price_post_baseline = mc_array - share_array_baseline/partial_array_baseline\n",
    "\n",
    "            # break or update\n",
    "            if (abs(price_post_baseline - price_pre_baseline)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                price_pre_baseline = price_post_baseline\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        # (2) calculate post merge price: p^**\n",
    "        count = 0\n",
    "        while (count <= 50):\n",
    "            # calculate share and partial derivative\n",
    "            share_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data)[0]\n",
    "            partial_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data)[1]\n",
    "\n",
    "            # calculate post price for j = 3,4,5\n",
    "            price_post_counterfactual345 = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
    "            price_post_345 = list(price_post_counterfactual345)[2:]\n",
    "\n",
    "            # calculate post price for j = 1,2 \n",
    "            s1 = share_array_counterfactual[0]\n",
    "            s2 = share_array_counterfactual[1]\n",
    "            s1_over_p1 = partial_array_counterfactual[0]\n",
    "            s2_over_p2 = partial_array_counterfactual[1]\n",
    "            s2_over_p1 = cross_elasticity(price_pre_counterfactual, work_data)\n",
    "            s1_over_p2 = s2_over_p1\n",
    "            price_post1 = mc_array[0] - s1/s1_over_p1 - (price_pre_counterfactual[1] - mc_array[1])*(s2_over_p1/s1_over_p1)\n",
    "            price_post2 = mc_array[1] - s2/s2_over_p2 - (price_pre_counterfactual[0] - mc_array[0])*(s1_over_p2/s2_over_p2)\n",
    "\n",
    "            # update price\n",
    "            price_post_counterfactual = np.array([price_post1, price_post2] + price_post_345)\n",
    "\n",
    "\n",
    "            # break or update\n",
    "            if (abs(price_post_counterfactual - price_pre_counterfactual)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                price_pre_counterfactual = price_post_counterfactual\n",
    "\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        eqm_price_pre_merge = price_post_baseline\n",
    "        eqm_price_post_merge = price_post_counterfactual\n",
    "\n",
    "\n",
    "        # (3) calculate pi under pre_merge and post_merge price\n",
    "        pre_merge_share = share_partial_calculator_baseline(eqm_price_pre_merge, work_data)[0]\n",
    "        pre_merge_pi = (eqm_price_pre_merge - mc_array)*pre_merge_share #array\n",
    "\n",
    "        post_merge_share = share_partial_calculator_baseline(eqm_price_post_merge, work_data)[0]\n",
    "        post_merge_pi = (eqm_price_post_merge - mc_array)*post_merge_share #array\n",
    "\n",
    "        delta_pi.append(post_merge_pi - pre_merge_pi)\n",
    "        eqm_price_before.append(eqm_price_pre_merge)\n",
    "        eqm_price_after.append(eqm_price_post_merge)\n",
    "\n",
    "    return np.array(delta_pi).mean(0), np.array(eqm_price_before).mean(0), np.array(eqm_price_after).mean(0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pi13(t):\n",
    "    # STEP2&3 calculate the expected profit change\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "    delta_pi = []\n",
    "    eqm_price_before = []\n",
    "    eqm_price_after = []\n",
    "    \n",
    "    R = 100\n",
    "    for r in range(R):    \n",
    "        xi_1_random = random.sample(list(xi_hat_1), 1)[0]\n",
    "        xi_2_random = random.sample(list(xi_hat_2), 1)[0]\n",
    "        xi_3_random = random.sample(list(xi_hat_3), 1)[0]\n",
    "        xi_4_random = random.sample(list(xi_hat_4), 1)[0]\n",
    "        xi_5_random = random.sample(list(xi_hat_5), 1)[0]\n",
    "\n",
    "        e_1_random = random.sample(list(e_hat_1), 1)[0]\n",
    "        e_2_random = random.sample(list(e_hat_2), 1)[0]\n",
    "        e_3_random = random.sample(list(e_hat_3), 1)[0]\n",
    "        e_4_random = random.sample(list(e_hat_4), 1)[0]\n",
    "        e_5_random = random.sample(list(e_hat_5), 1)[0]\n",
    "\n",
    "        xi_random_array = np.array([xi_1_random, xi_2_random, xi_3_random, xi_4_random, xi_5_random])\n",
    "        e_random_array = np.array([e_1_random, e_2_random, e_3_random, e_4_random, e_5_random])\n",
    "        pd.set_option('mode.chained_assignment',  None)\n",
    "        work_data['xi'] = xi_random_array\n",
    "        work_data['e'] = e_random_array\n",
    "\n",
    "        epsilon = 1e-14\n",
    "        price_pre_baseline = np.array([4,4,4,4,4])\n",
    "        price_pre_counterfactual = np.array([4,4,4,4,4])\n",
    "        mc_array = omega_0_hat + omega_1_hat*work_data['x'].values + omega_2_hat*work_data['z_1'].values + omega_3_hat*work_data['z_2'].values + work_data['e'].values\n",
    "\n",
    "\n",
    "        # (1) calculate baseline price: p^*\n",
    "        count = 0\n",
    "        while (count <= 50):\n",
    "            # calculate share and partial derivative\n",
    "            share_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[0]\n",
    "            partial_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[1]\n",
    "\n",
    "            # calculate post price\n",
    "            price_post_baseline = mc_array - share_array_baseline/partial_array_baseline\n",
    "\n",
    "            # break or update\n",
    "            if (abs(price_post_baseline - price_pre_baseline)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                price_pre_baseline = price_post_baseline\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        # (2) calculate post merge price: p^**\n",
    "        count = 0\n",
    "        while (count <= 50):\n",
    "            # calculate share and partial derivative\n",
    "            share_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data)[0]\n",
    "            partial_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data)[1]\n",
    "\n",
    "            # calculate post price for j = 2,4,5\n",
    "            price_post_counterfactual245 = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
    "            price_post2 = price_post_counterfactual245[1]\n",
    "            price_post4 = price_post_counterfactual245[3]\n",
    "            price_post5 = price_post_counterfactual245[4]\n",
    "\n",
    "            # calculate post price for j = 1,3 \n",
    "            s1 = share_array_counterfactual[0]\n",
    "            s2 = share_array_counterfactual[2]\n",
    "            s1_over_p1 = partial_array_counterfactual[0]\n",
    "            s2_over_p2 = partial_array_counterfactual[2]\n",
    "            s2_over_p1 = cross_elasticity13(price_pre_counterfactual, work_data)\n",
    "            s1_over_p2 = s2_over_p1\n",
    "            price_post1 = mc_array[0] - s1/s1_over_p1 - (price_pre_counterfactual[2] - mc_array[2])*(s2_over_p1/s1_over_p1)\n",
    "            price_post3 = mc_array[2] - s2/s2_over_p2 - (price_pre_counterfactual[0] - mc_array[0])*(s1_over_p2/s2_over_p2)\n",
    "\n",
    "            # update price\n",
    "            price_post_counterfactual = np.array([price_post1, price_post2, price_post3, price_post4, price_post5])\n",
    "\n",
    "\n",
    "            # break or update\n",
    "            if (abs(price_post_counterfactual - price_pre_counterfactual)).max() < epsilon:\n",
    "                break\n",
    "            else: \n",
    "                price_pre_counterfactual = price_post_counterfactual\n",
    "\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        eqm_price_pre_merge = price_post_baseline\n",
    "        eqm_price_post_merge = price_post_counterfactual\n",
    "\n",
    "\n",
    "        # (3) calculate pi under pre_merge and post_merge price\n",
    "        pre_merge_share = share_partial_calculator_baseline(eqm_price_pre_merge, work_data)[0]\n",
    "        pre_merge_pi = (eqm_price_pre_merge - mc_array)*pre_merge_share #array\n",
    "\n",
    "        post_merge_share = share_partial_calculator_baseline(eqm_price_post_merge, work_data)[0]\n",
    "        post_merge_pi = (eqm_price_post_merge - mc_array)*post_merge_share #array\n",
    "\n",
    "        delta_pi.append(post_merge_pi - pre_merge_pi)\n",
    "        eqm_price_before.append(eqm_price_pre_merge)\n",
    "        eqm_price_after.append(eqm_price_post_merge)\n",
    "\n",
    "    return np.array(delta_pi).mean(0), np.array(eqm_price_before).mean(0), np.array(eqm_price_after).mean(0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected change in profit and price can be calculated by the following functions\n",
    "def outcome_calculator_merge12():\n",
    "    T = 20\n",
    "    pi_vec = []\n",
    "    price_before_vec = []\n",
    "    price_after_vec = []\n",
    "    for t in range(1, T+1):\n",
    "        delta_pi, price_before, price_after = merge_pi12(t)\n",
    "        pi_vec.append(delta_pi)\n",
    "        price_before_vec.append(price_before)\n",
    "        price_after_vec.append(price_after)\n",
    "        \n",
    "\n",
    "    return np.array(pi_vec).mean(0), np.array(price_before_vec).mean(0), np.array(price_after_vec).mean(0)\n",
    "\n",
    "def outcome_calculator_merge13():\n",
    "    T = 20\n",
    "    pi_vec = []\n",
    "    price_before_vec = []\n",
    "    price_after_vec = []\n",
    "    for t in range(1, T+1):\n",
    "        delta_pi, price_before, price_after = merge_pi13(t)\n",
    "        pi_vec.append(delta_pi)\n",
    "        price_before_vec.append(price_before)\n",
    "        price_after_vec.append(price_after)\n",
    "        \n",
    "\n",
    "    return np.array(pi_vec).mean(0), np.array(price_before_vec).mean(0), np.array(price_after_vec).mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_pi_merge12, price_before_merge12, price_after_merge12 = outcome_calculator_merge12()\n",
    "delta_pi_merge13, price_before_merge13, price_after_merge13 = outcome_calculator_merge13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00152442, 0.00070008, 0.00298533, 0.00043191, 0.00093233])"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_pi_merge12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00244075, 0.00327294, 0.00584712, 0.00108887, 0.00221819])"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_pi_merge13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5842931  2.52181093 0.09963794 0.03374281 0.04871237]\n",
      "[4.3548315  0.14029352 2.86891934 0.08821924 0.11975768]\n"
     ]
    }
   ],
   "source": [
    "print(100*(price_after_merge12-price_before_merge12)/price_before_merge12)\n",
    "print(100*(price_after_merge13-price_before_merge13)/price_before_merge13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.32640469 8.28112529 8.49702877 7.97260304 7.89171984]\n",
      "[8.45831935 8.48995961 8.50549504 7.97529322 7.89556409]\n"
     ]
    }
   ],
   "source": [
    "print(price_before_merge12)\n",
    "print(price_after_merge12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.34961883 8.2649556  8.48337404 7.97087306 7.89416022]\n",
      "[8.71323066 8.27655079 8.7267552  7.9779049  7.90361409]\n"
     ]
    }
   ],
   "source": [
    "print(price_before_merge13)\n",
    "print(price_after_merge13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_before_merge(xi, e, work_data):\n",
    "    alpha_hat_list_prime = alpha_hat_list.copy()\n",
    "    work_data_temp = work_data.copy()\n",
    "    work_data_temp['xi'] = xi\n",
    "    work_data_temp['e'] = e\n",
    "    \n",
    "    price_pre_baseline = np.array([4,4,4,4,4])\n",
    "    mc_array = omega_0_hat + omega_1_hat*work_data_temp['x'].values + omega_2_hat*work_data_temp['z_1'].values + omega_3_hat*work_data_temp['z_2'].values + work_data_temp['e'].values\n",
    "\n",
    "    # (1) calculate baseline price: p^*\n",
    "    count = 0\n",
    "    while (count <= 50):\n",
    "        # calculate share and partial derivative\n",
    "        share_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[0]\n",
    "        partial_array_baseline = share_partial_calculator_baseline(price_pre_baseline, work_data)[1]\n",
    "\n",
    "        # calculate post price\n",
    "        price_post_baseline = mc_array - share_array_baseline/partial_array_baseline\n",
    "\n",
    "        # break or update\n",
    "        if (abs(price_post_baseline - price_pre_baseline)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            price_pre_baseline = price_post_baseline\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    eqm_price = price_post_baseline\n",
    "    \n",
    "    # (2) calculate exponential sum\n",
    "    exp_sum = 0\n",
    "    for k in range(5):\n",
    "        x_k = work_data_temp['x'].values[k]\n",
    "        p_k = eqm_price[k]\n",
    "        xi_k = work_data_temp['xi'].values[k]\n",
    "        u_k_true = alpha_hat_list_prime[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "        exp_sum += np.exp(u_k_true)\n",
    "\n",
    "    # (3) calculate the average\n",
    "    numerator = np.log(1+exp_sum)\n",
    "    denominator = beta_bar_hat + sigma_beta_hat*nu_perseon_1000\n",
    "    CW_before = -(numerator/denominator).mean()\n",
    "\n",
    "    return CW_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_after_merge12(xi, e, work_data):\n",
    "    '''\n",
    "    xi: sampled 5X1 vector of xi \n",
    "    e : sampled 5X1 vector of e\n",
    "    work_data: certain market t's data\n",
    "    j : counterfactual product\n",
    "    '''\n",
    "    alpha_hat_list_prime = alpha_hat_list.copy()\n",
    "    work_data_temp = work_data.copy()\n",
    "    work_data_temp['xi'] = xi\n",
    "    work_data_temp['e'] = e\n",
    "    \n",
    "    price_pre_counterfactual = np.array([4,4,4,4,4])\n",
    "    mc_array = omega_0_hat + omega_1_hat*work_data_temp['x'].values + omega_2_hat*work_data_temp['z_1'].values + omega_3_hat*work_data_temp['z_2'].values + work_data_temp['e'].values\n",
    "    \n",
    "    # (2) calculate post merge price: p^**\n",
    "    count = 0\n",
    "    while (count <= 50):\n",
    "        # calculate share and partial derivative\n",
    "        share_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data_temp)[0]\n",
    "        partial_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data_temp)[1]\n",
    "\n",
    "        # calculate post price for j = 3,4,5\n",
    "        price_post_counterfactual345 = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
    "        price_post_345 = list(price_post_counterfactual345)[2:]\n",
    "\n",
    "        # calculate post price for j = 1,2 \n",
    "        s1 = share_array_counterfactual[0]\n",
    "        s2 = share_array_counterfactual[1]\n",
    "        s1_over_p1 = partial_array_counterfactual[0]\n",
    "        s2_over_p2 = partial_array_counterfactual[1]\n",
    "        s2_over_p1 = cross_elasticity(price_pre_counterfactual, work_data)\n",
    "        s1_over_p2 = s2_over_p1\n",
    "        price_post1 = mc_array[0] - s1/s1_over_p1 - (price_pre_counterfactual[1] - mc_array[1])*(s2_over_p1/s1_over_p1)\n",
    "        price_post2 = mc_array[1] - s2/s2_over_p2 - (price_pre_counterfactual[0] - mc_array[0])*(s1_over_p2/s2_over_p2)\n",
    "\n",
    "        # update price\n",
    "        price_post_counterfactual = np.array([price_post1, price_post2] + price_post_345)\n",
    "\n",
    "\n",
    "        # break or update\n",
    "        if (abs(price_post_counterfactual - price_pre_counterfactual)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            price_pre_counterfactual = price_post_counterfactual\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    eqm_price = price_post_counterfactual\n",
    "    if math.isnan(eqm_price[0]) == True:\n",
    "        CW_after = 10000\n",
    "\n",
    "    # (2) calculate exponential sum\n",
    "    else:\n",
    "        exp_sum = 0\n",
    "        for k in range(5):\n",
    "            x_k = work_data_temp['x'].values[k]\n",
    "            p_k = eqm_price[k]\n",
    "            xi_k = work_data_temp['xi'].values[k]\n",
    "            u_k_true = alpha_hat_list_prime[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "            exp_sum += np.exp(u_k_true)\n",
    "\n",
    "        # (3) calculate the average\n",
    "        numerator = np.log(1+exp_sum)\n",
    "        denominator = beta_bar_hat + sigma_beta_hat*nu_perseon_1000\n",
    "        CW_after = -(numerator/denominator).mean()\n",
    "\n",
    "    return CW_after\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_after_merge13(xi, e, work_data):\n",
    "    '''\n",
    "    xi: sampled 5X1 vector of xi \n",
    "    e : sampled 5X1 vector of e\n",
    "    work_data: certain market t's data\n",
    "    j : counterfactual product\n",
    "    '''\n",
    "    alpha_hat_list_prime = alpha_hat_list.copy()\n",
    "    work_data_temp = work_data.copy()\n",
    "    work_data_temp['xi'] = xi\n",
    "    work_data_temp['e'] = e\n",
    "    \n",
    "    price_pre_counterfactual = np.array([4,4,4,4,4])\n",
    "    mc_array = omega_0_hat + omega_1_hat*work_data_temp['x'].values + omega_2_hat*work_data_temp['z_1'].values + omega_3_hat*work_data_temp['z_2'].values + work_data_temp['e'].values\n",
    "    \n",
    "    # (2) calculate post merge price: p^**\n",
    "    count = 0\n",
    "    while (count <= 50):\n",
    "        # calculate share and partial derivative\n",
    "        share_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data)[0]\n",
    "        partial_array_counterfactual = share_partial_calculator_baseline(price_pre_counterfactual, work_data)[1]\n",
    "\n",
    "        # calculate post price for j = 2,4,5\n",
    "        price_post_counterfactual245 = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
    "        price_post2 = price_post_counterfactual245[1]\n",
    "        price_post4 = price_post_counterfactual245[3]\n",
    "        price_post5 = price_post_counterfactual245[4]\n",
    "\n",
    "        # calculate post price for j = 1,3 \n",
    "        s1 = share_array_counterfactual[0]\n",
    "        s2 = share_array_counterfactual[2]\n",
    "        s1_over_p1 = partial_array_counterfactual[0]\n",
    "        s2_over_p2 = partial_array_counterfactual[2]\n",
    "        s2_over_p1 = cross_elasticity13(price_pre_counterfactual, work_data)\n",
    "        s1_over_p2 = s2_over_p1\n",
    "        price_post1 = mc_array[0] - s1/s1_over_p1 - (price_pre_counterfactual[2] - mc_array[2])*(s2_over_p1/s1_over_p1)\n",
    "        price_post3 = mc_array[2] - s2/s2_over_p2 - (price_pre_counterfactual[0] - mc_array[0])*(s1_over_p2/s2_over_p2)\n",
    "\n",
    "        # update price\n",
    "        price_post_counterfactual = np.array([price_post1, price_post2, price_post3, price_post4, price_post5])\n",
    "\n",
    "\n",
    "        # break or update\n",
    "        if (abs(price_post_counterfactual - price_pre_counterfactual)).max() < epsilon:\n",
    "            break\n",
    "        else: \n",
    "            price_pre_counterfactual = price_post_counterfactual\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    eqm_price = price_post_counterfactual\n",
    "    if math.isnan(eqm_price[0]) == True:\n",
    "        CW_after = 10000\n",
    "\n",
    "    # (2) calculate exponential sum\n",
    "    else:\n",
    "        exp_sum = 0\n",
    "        for k in range(5):\n",
    "            x_k = work_data_temp['x'].values[k]\n",
    "            p_k = eqm_price[k]\n",
    "            xi_k = work_data_temp['xi'].values[k]\n",
    "            u_k_true = alpha_hat_list_prime[k] + gamma_hat*x_k + beta_bar_hat*p_k + xi_k + sigma_beta_hat*p_k*nu_perseon_1000        \n",
    "            exp_sum += np.exp(u_k_true)\n",
    "\n",
    "        # (3) calculate the average\n",
    "        numerator = np.log(1+exp_sum)\n",
    "        denominator = beta_bar_hat + sigma_beta_hat*nu_perseon_1000\n",
    "        CW_after = -(numerator/denominator).mean()\n",
    "\n",
    "    return CW_after\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_change_merge12(t):\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "    CW_delta = 0    \n",
    "    valid_count = 0\n",
    "    count = 0\n",
    "    while(count <= 200):\n",
    "        # randomly draw xi and e \n",
    "        xi_1_random = random.sample(list(xi_hat_1), 1)[0]\n",
    "        xi_2_random = random.sample(list(xi_hat_2), 1)[0]\n",
    "        xi_3_random = random.sample(list(xi_hat_3), 1)[0]\n",
    "        xi_4_random = random.sample(list(xi_hat_4), 1)[0]\n",
    "        xi_5_random = random.sample(list(xi_hat_5), 1)[0]\n",
    "\n",
    "        e_1_random = random.sample(list(e_hat_1), 1)[0]\n",
    "        e_2_random = random.sample(list(e_hat_2), 1)[0]\n",
    "        e_3_random = random.sample(list(e_hat_3), 1)[0]\n",
    "        e_4_random = random.sample(list(e_hat_4), 1)[0]\n",
    "        e_5_random = random.sample(list(e_hat_5), 1)[0]\n",
    "\n",
    "        xi_random_array = np.array([xi_1_random, xi_2_random, xi_3_random, xi_4_random, xi_5_random])\n",
    "        e_random_array = np.array([e_1_random, e_2_random, e_3_random, e_4_random, e_5_random])\n",
    "        pd.set_option('mode.chained_assignment',  None)\n",
    "        \n",
    "        CW_post = CW_after_merge12(xi_random_array, e_random_array, work_data)\n",
    "        CW_pre = CW_before_merge(xi_random_array, e_random_array, work_data)\n",
    "\n",
    "        if CW_post != 10000: \n",
    "            CW_delta += (CW_post - CW_pre)\n",
    "            valid_count += 1\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    return CW_delta/valid_count\n",
    "\n",
    "\n",
    "def CW_change_merge13(t):\n",
    "    work_data = df_master.loc[df_master['t'] == t]\n",
    "\n",
    "    CW_delta = 0    \n",
    "    valid_count = 0\n",
    "    count = 0\n",
    "    while(count <= 200):\n",
    "        # randomly draw xi and e \n",
    "        xi_1_random = random.sample(list(xi_hat_1), 1)[0]\n",
    "        xi_2_random = random.sample(list(xi_hat_2), 1)[0]\n",
    "        xi_3_random = random.sample(list(xi_hat_3), 1)[0]\n",
    "        xi_4_random = random.sample(list(xi_hat_4), 1)[0]\n",
    "        xi_5_random = random.sample(list(xi_hat_5), 1)[0]\n",
    "\n",
    "        e_1_random = random.sample(list(e_hat_1), 1)[0]\n",
    "        e_2_random = random.sample(list(e_hat_2), 1)[0]\n",
    "        e_3_random = random.sample(list(e_hat_3), 1)[0]\n",
    "        e_4_random = random.sample(list(e_hat_4), 1)[0]\n",
    "        e_5_random = random.sample(list(e_hat_5), 1)[0]\n",
    "\n",
    "        xi_random_array = np.array([xi_1_random, xi_2_random, xi_3_random, xi_4_random, xi_5_random])\n",
    "        e_random_array = np.array([e_1_random, e_2_random, e_3_random, e_4_random, e_5_random])\n",
    "        pd.set_option('mode.chained_assignment',  None)\n",
    "        \n",
    "        CW_post = CW_after_merge13(xi_random_array, e_random_array, work_data)\n",
    "        CW_pre = CW_before_merge(xi_random_array, e_random_array, work_data)\n",
    "\n",
    "        if CW_post != 10000: \n",
    "            CW_delta += (CW_post - CW_pre)\n",
    "            valid_count += 1\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    return CW_delta/valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_change_calculator_merge12():\n",
    "    CW_change_list = [CW_change_merge12(t) for t in range(1,101)]\n",
    "    return np.array(CW_change_list).mean()\n",
    "\n",
    "def CW_change_calculator_merge13():\n",
    "    CW_change_list = [CW_change_merge13(t) for t in range(1,101)]\n",
    "    return np.array(CW_change_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/gh9mkkld0rl0y75bn83mfm980000gn/T/ipykernel_61255/873404647.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  price_post_counterfactual345 = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
      "/var/folders/_m/gh9mkkld0rl0y75bn83mfm980000gn/T/ipykernel_61255/873404647.py:35: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  price_post2 = mc_array[1] - s2/s2_over_p2 - (price_pre_counterfactual[0] - mc_array[0])*(s1_over_p2/s2_over_p2)\n",
      "/var/folders/_m/gh9mkkld0rl0y75bn83mfm980000gn/T/ipykernel_61255/873404647.py:34: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  price_post1 = mc_array[0] - s1/s1_over_p1 - (price_pre_counterfactual[1] - mc_array[1])*(s2_over_p1/s1_over_p1)\n",
      "/var/folders/_m/gh9mkkld0rl0y75bn83mfm980000gn/T/ipykernel_61255/873404647.py:24: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  price_post_counterfactual345 = mc_array - share_array_counterfactual/partial_array_counterfactual\n",
      "/var/folders/_m/gh9mkkld0rl0y75bn83mfm980000gn/T/ipykernel_61255/873404647.py:34: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  price_post1 = mc_array[0] - s1/s1_over_p1 - (price_pre_counterfactual[1] - mc_array[1])*(s2_over_p1/s1_over_p1)\n",
      "/var/folders/_m/gh9mkkld0rl0y75bn83mfm980000gn/T/ipykernel_61255/873404647.py:35: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  price_post2 = mc_array[1] - s2/s2_over_p2 - (price_pre_counterfactual[0] - mc_array[0])*(s1_over_p2/s2_over_p2)\n"
     ]
    }
   ],
   "source": [
    "delta_CW_merge12 = CW_change_calculator_merge12()\n",
    "delta_CW_merge13 = CW_change_calculator_merge13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumer welfare change from firm 1 and 2's merge:  -0.04096952072843748\n",
      "consumer welfare change from firm 1 and 3's merge:  -0.04989146004323239\n"
     ]
    }
   ],
   "source": [
    "print(\"consumer welfare change from firm 1 and 2's merge: \", delta_CW_merge12)\n",
    "print(\"consumer welfare change from firm 1 and 3's merge: \", delta_CW_merge13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4058ae7d101d4d34f9fe782bbb6316095f0ede2a1a2e6a1564fc2428590ee83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
